{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ACM SRE Site reliability engineering (SRE) is a software engineering approach to IT operations. SRE teams use software as a tool to manage systems, solve problems, and automate operations tasks. SRE takes the tasks that have historically been done by operations teams, often manually, and instead gives them to engineers or ops teams who use software and automation to solve problems and manage production systems. SRE is a valuable practice when creating scalable and highly reliable software systems. It helps you manage large systems through code, which is more scalable and sustainable for sysadmins managing thousands or hundreds of thousands of machines. The concept of site reliability engineering comes from the Google engineering team and is credited to Ben Treynor Sloss. SRE helps teams find a balance between releasing new features and making sure that they are reliable for users. Standardization and automation are 2 important components of the SRE model. Site reliability engineers should always be looking for ways to enhance and automate operations tasks. In this way, SRE helps to improve the reliability of a system today, while also improving it as it grows over time. SRE supports teams who are moving from a traditional approach to IT operations to a cloud-native approach. From: https://www.redhat.com/en/topics/devops/what-is-sre The content here is an ongoing process. Our ACM SRE team will share our work with the RHACM community, and feedback is always welcome!","title":"Welcome to ACM SRE"},{"location":"design/","text":"Design Decisions Preface: https://www.redhat.com/en/blog/how-does-red-hat-support-day-2-operations Day 0 Operations Planning - How do we do Ops Project: AOC We use a multiple github repository to allow us to deploy and operate RHACM. For application and configuration data, we define kustomization overlays to cover each distinct deployment targets. Targets make up different stage environments or different region environments. We use Ansible Automation Platform to trigger Ansible Playbooks to build the infrastructure that make up the management components. This includes the ansible playbook to deploy OCP as a private cluster on Azure. We also define ansible playbooks to import AKS clusters into ACM. We need to support multiple ACM deployments across multiple regions. To limit costs, We support a two stage deployment workflow: development and production. Developers are still able to deploy sandbox environments, using an internal RHACM infrastructure cluster, with has support for cluster pools. We develop in the cloud. For the AOC project we use gitops with Openshift Gitops to deploy RHACM workload and configuration data. The image below is a sample the current set of ArgoCD applications we have defined. Project: KCP The source of the configuration for this cluster is held in the Openshift-pipelines repository, directly in the kcp-sgs-pipelines directory. The source for autoscale hypershift is also held in this repository in the autoscale-hypershift directory. This project leverages an existing RHACM infrastructure to deploy the public OCP clusters into which we will deploy the RHACM instance to support the project. The initial OCP clusters are detached from the infrastrucutre RHACM. The deployment of RHACM into OCP clusters will be managed by Openshift Pipelines. Openshift Pipeline will deploy Openshift Gitops, and similar to the AOC project, we will use Argocd to handle the rollout of ACM, the ACM configuration, the Singapore controller, and cluster registration controller. Once RHACM is successfully installed, the pipeline will configure HyperShift on the local cluster, converting the local cluster managedcluster into a HyperShift Hosting Cluster. The KCP cluster will be configured with 3 HyperShiftDeployments on its localcluster hosting cluster, 1 Azure and 1 AWS HyperShiftDeployment are configured to target KCP Stable, whereas the last AWS HyperShiftDeployment is configured to target KCP unstable. Autoscale HyperShift is deployed on this cluster as well, to ensure that HyperShiftDeployments are scaled up/down automatically on a preexisting schedule to aid in cost savings efforts. As of now, HyperShiftDeployments will scale down every Friday at Midnight, and scale up every Monday at 1 AM. A label of autoscale-hypershift=skip can be added to the HyperShiftDeployment to ensure it is not affected by the automated scaling When scaling down, autoscaling is turned off and the replicaCount is set to 1 When scaling up, autoscaling is turned on and the minimum replicas is set to 2 and max to 5 Observability is installed on this cluster and AlertManager is configured to forward warning and critical alerts to slack and GitHub. PagerDuty will alert on critical alerts as well. Commonality across all RHACM deployments TBD Planning - Sizing One ACM Hub / Bastion Host per Region Currently, we support AKS cluster deployed across the NA region. A single ACM hub cluster will be available to import and manage all the AKS clusters in the North America region. The EMEA region will consist of Europe, Middle East, and Africa countries. DNS Setup Project: AOC The basic deployment of the OCP platform hosting the hub is private, including the DNS setup. On top of that, we add a Public DNS zone containing: - record pointing to the public IP of the bastion VM - record pointing to the private IP of the Load Balancer frontend IPs for api and *.apps This is used for certificate validation purposes and to avoid the need to go through the bastion also for the DNS traffic. The expected domain name is: <api|*.apps>.<hubname>.<AOC base domain> where AOC base domain can be: na.mgmtdev.ansiblecloud.com -> Development deployment in NA region emea.mgmt.ansiblecloud.com -> Production deployment in EMEA region Zones mgmtdev.ansiblecloud.com na.mgmt.ansiblecloud.com emea.mgmt.ansiblecloud.com Day 1 Operations New Environment Deployments The Ansible project ocp4-azure-ipi was forked and is used to deploy OCP clusters. Red Hat Ansibile Automation Platform is used to perform the Day 1 deployment of the Openshift Clusters. A series of Ansible playbooks from acm-aap-aas-operations are used to: configure the Bastion VM setup Public DNS configuration bootstrap the Openshift Gitops. Openshift Gitops handles rolling out and maintain the set of application and configuration to the ACM hub cluster, or local cluster. ACM policy is used for maintain configuration across the fleet of managed clusters. Project: AOC graph LR AAP --> |OCP IPI| OCP AAP --> |Bastion/DNS setup| OCP[OCP & RHACM] AAP --> |Openshift GitOps| OCP[OCP & RHACM] OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET Project: KCP graph LR RHACM[RHACM*] --> |OCP IPI| OCP[OCP & RHACM] PIPELINE --> |Openshift GitOps| OCP OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET Day 2 Operations Importing AKS Clusters AKS clusters are imported into the ACM hub as a Day 2 operation using an ansible playbook. The playbook is idempotent and runs against an inventory of AKS clusters. The inventory is dyanmic, where targets can be added and removed. In this way, a single playbook can be used against inventories from different stages--dev and production, or different regions. The following tasks are performed on each cluster: Setup networking to allow the AKS cluster to access the ACM k8s endpoint. Setup networking to allow the AAP cluster to access the AKS k8s endpoint. Generate the ManageCluster CR on the ACM hub. Read and apply the generated import secret on the AKS cluster. Reference See details at 1.8.2. Importing a managed cluster with the CLI As new AKS clusters come online, the ansible playbook running on a schedule will iterate over the inventory of clusters and import the clusters into ACM. Note If it takes 5m to run the import playbook from start to end for a single AKS cluster, then iterating over 100 AKS clusters, will take 500m or 8.3 hours, as a rough estimate. Enabling parallism will reduce the total time. Currently, the import procedure follows the out of the box import procedure, using a Service Account with cluster-admin rolebinding to generate the managed cluster namespace, and create the Managed Cluster CR. The best practice is always to use the mininal privilege as possible, so we question, if customers in general are ok with using a cluster-admin role to import all their clusters. An alternative import procedure is available, that uses a generic import payload to be applied on the AKS cluster, and a service account with minimal privledge. This requires the ACM hub to be configured to generate a generic payload. Using this procedure will simplify the import procedure by not having to access the ACM hub cluster during the import procedure. We would just need to access the target cluster. This alternative process will still be a Day 2 operation. Cleaning Up Unknown Managed Clusters Today, most of the managed clusters that we see with Unknown states are clusters that have their managed applications deleted by customers. The current number has been manageable by manually detaching the clusters through the UI. To automate this task, we can follow this psuedo code: get the list of managed applications from the dynamic inventory get the list of managed clusters by their label, managed applications. the obsolete managed clusters will be the managed clusters with labels not in the dyanmic inventory list of managed applications. Backup and Restore | Disaster Recovery TBD Upgrade - OCP We are following the following OCP upgrade policy: The ACM SRE maintenance window is between Sunday night (CST) and Wednesday morning (CST). We are following OpenShift stable upgrade channel OCP minor version release upgrades will happen during this window. We will deploy the latest version, weekly. OCP major version release upgrades will happen during this window. We will upgrade to the next release, 1 week after it is available. Development stage is upgraded first. Production(s) stage is upgraded next. Gitops procedure for upgrading OCP This is driven by an ACM Policy that applies the expected ClusterVersion Custom Resource. Once the manifest is applied, OCP will orchestrate the update until it finishes. Upgrade - ACM The upgrade of RHACM will happen during the upgrade maintenance window from Sunday night (CST) to Wedneday morning (CST). This allows us to provide business hour support across the maintenance windown. We will upgrade ACM on the maintenance window following the release of RHACM. Development stage is upgraded first. Production(s) stage is upgraded next. Gitops procedure for upgrading RHACM 2.4 to RHACM 2.5 There is a known blocker issue for upgrading RHACM from 2.4 to 2.5 when the cluster proxy addon is enabled. Before upgrading from 2.4 to 2.5 we also need to disable the backup feature. The following steps includes the workaround to allow the upgrade with Gitops. Create PR update MultiClusterHub CR to disable proxy addon spec : enableClusterProxyAddon : false Create PR to update ACM subscription spec : channel : release-2.5 installPlanApproval : Manual name : advanced-cluster-management source : redhat-operators sourceNamespace : openshift-marketplace Login to the OCP console to review and approve the install plan for release 2.5 upgrade. After the RHACM 2.5 upgrade completes successfully, create PR to update MultiClusterHub CR to re-enable the cluster proxy addon. apiVersion : operator.open-cluster-management.io/v1 kind : MultiClusterHub metadata : name : multiclusterhub namespace : open-cluster-management spec : overrides : components : - enabled : true name : cluster-proxy-addon - enabled : true name : multiclusterhub-repo - enabled : true name : search - enabled : true name : management-ingress - enabled : true name : console - enabled : true name : insights - enabled : true name : grc - enabled : true name : cluster-lifecycle - enabled : true name : volsync - enabled : true name : multicluster-engine - enabled : false name : cluster-backup Manually delete the secret cluster-proxy-signer and let cluster-proxy-addon-manager to refresh it. oc delete secret cluster-proxy-signer -n open-cluster-management Alerts and Alert Management TBD Observability TBD Certificate management Certificates on the cluster are signed by Let's Encrypt CA for api and default ingress (*.apps). Certificates are handled by cert-manager which is configured to use DNS-01 challenge. This is the reason for having a DNS Public Zone . Certificates lasts 90 days and are automatically renewed every 30 days. An ACM policy monitors the certificate expiration and local cluster will pass to non-compliant status if there is any certificate which expire in less than 25 days.","title":"Design Decisions"},{"location":"design/#day-0-operations","text":"","title":"Day 0 Operations"},{"location":"design/#planning-how-do-we-do-ops","text":"","title":"Planning - How do we do Ops"},{"location":"design/#project-aoc","text":"We use a multiple github repository to allow us to deploy and operate RHACM. For application and configuration data, we define kustomization overlays to cover each distinct deployment targets. Targets make up different stage environments or different region environments. We use Ansible Automation Platform to trigger Ansible Playbooks to build the infrastructure that make up the management components. This includes the ansible playbook to deploy OCP as a private cluster on Azure. We also define ansible playbooks to import AKS clusters into ACM. We need to support multiple ACM deployments across multiple regions. To limit costs, We support a two stage deployment workflow: development and production. Developers are still able to deploy sandbox environments, using an internal RHACM infrastructure cluster, with has support for cluster pools. We develop in the cloud. For the AOC project we use gitops with Openshift Gitops to deploy RHACM workload and configuration data. The image below is a sample the current set of ArgoCD applications we have defined.","title":"Project: AOC"},{"location":"design/#project-kcp","text":"The source of the configuration for this cluster is held in the Openshift-pipelines repository, directly in the kcp-sgs-pipelines directory. The source for autoscale hypershift is also held in this repository in the autoscale-hypershift directory. This project leverages an existing RHACM infrastructure to deploy the public OCP clusters into which we will deploy the RHACM instance to support the project. The initial OCP clusters are detached from the infrastrucutre RHACM. The deployment of RHACM into OCP clusters will be managed by Openshift Pipelines. Openshift Pipeline will deploy Openshift Gitops, and similar to the AOC project, we will use Argocd to handle the rollout of ACM, the ACM configuration, the Singapore controller, and cluster registration controller. Once RHACM is successfully installed, the pipeline will configure HyperShift on the local cluster, converting the local cluster managedcluster into a HyperShift Hosting Cluster. The KCP cluster will be configured with 3 HyperShiftDeployments on its localcluster hosting cluster, 1 Azure and 1 AWS HyperShiftDeployment are configured to target KCP Stable, whereas the last AWS HyperShiftDeployment is configured to target KCP unstable. Autoscale HyperShift is deployed on this cluster as well, to ensure that HyperShiftDeployments are scaled up/down automatically on a preexisting schedule to aid in cost savings efforts. As of now, HyperShiftDeployments will scale down every Friday at Midnight, and scale up every Monday at 1 AM. A label of autoscale-hypershift=skip can be added to the HyperShiftDeployment to ensure it is not affected by the automated scaling When scaling down, autoscaling is turned off and the replicaCount is set to 1 When scaling up, autoscaling is turned on and the minimum replicas is set to 2 and max to 5 Observability is installed on this cluster and AlertManager is configured to forward warning and critical alerts to slack and GitHub. PagerDuty will alert on critical alerts as well.","title":"Project: KCP"},{"location":"design/#commonality-across-all-rhacm-deployments","text":"TBD","title":"Commonality across all RHACM deployments"},{"location":"design/#planning-sizing","text":"","title":"Planning - Sizing"},{"location":"design/#one-acm-hub-bastion-host-per-region","text":"Currently, we support AKS cluster deployed across the NA region. A single ACM hub cluster will be available to import and manage all the AKS clusters in the North America region. The EMEA region will consist of Europe, Middle East, and Africa countries.","title":"One ACM Hub / Bastion Host per Region"},{"location":"design/#dns-setup","text":"","title":"DNS Setup"},{"location":"design/#project-aoc_1","text":"The basic deployment of the OCP platform hosting the hub is private, including the DNS setup. On top of that, we add a Public DNS zone containing: - record pointing to the public IP of the bastion VM - record pointing to the private IP of the Load Balancer frontend IPs for api and *.apps This is used for certificate validation purposes and to avoid the need to go through the bastion also for the DNS traffic. The expected domain name is: <api|*.apps>.<hubname>.<AOC base domain> where AOC base domain can be: na.mgmtdev.ansiblecloud.com -> Development deployment in NA region emea.mgmt.ansiblecloud.com -> Production deployment in EMEA region Zones mgmtdev.ansiblecloud.com na.mgmt.ansiblecloud.com emea.mgmt.ansiblecloud.com","title":"Project: AOC"},{"location":"design/#day-1-operations","text":"","title":"Day 1 Operations"},{"location":"design/#new-environment-deployments","text":"The Ansible project ocp4-azure-ipi was forked and is used to deploy OCP clusters. Red Hat Ansibile Automation Platform is used to perform the Day 1 deployment of the Openshift Clusters. A series of Ansible playbooks from acm-aap-aas-operations are used to: configure the Bastion VM setup Public DNS configuration bootstrap the Openshift Gitops. Openshift Gitops handles rolling out and maintain the set of application and configuration to the ACM hub cluster, or local cluster. ACM policy is used for maintain configuration across the fleet of managed clusters.","title":"New Environment Deployments"},{"location":"design/#project-aoc_2","text":"graph LR AAP --> |OCP IPI| OCP AAP --> |Bastion/DNS setup| OCP[OCP & RHACM] AAP --> |Openshift GitOps| OCP[OCP & RHACM] OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET","title":"Project: AOC"},{"location":"design/#project-kcp_1","text":"graph LR RHACM[RHACM*] --> |OCP IPI| OCP[OCP & RHACM] PIPELINE --> |Openshift GitOps| OCP OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET","title":"Project: KCP"},{"location":"design/#day-2-operations","text":"","title":"Day 2 Operations"},{"location":"design/#importing-aks-clusters","text":"AKS clusters are imported into the ACM hub as a Day 2 operation using an ansible playbook. The playbook is idempotent and runs against an inventory of AKS clusters. The inventory is dyanmic, where targets can be added and removed. In this way, a single playbook can be used against inventories from different stages--dev and production, or different regions. The following tasks are performed on each cluster: Setup networking to allow the AKS cluster to access the ACM k8s endpoint. Setup networking to allow the AAP cluster to access the AKS k8s endpoint. Generate the ManageCluster CR on the ACM hub. Read and apply the generated import secret on the AKS cluster. Reference See details at 1.8.2. Importing a managed cluster with the CLI As new AKS clusters come online, the ansible playbook running on a schedule will iterate over the inventory of clusters and import the clusters into ACM. Note If it takes 5m to run the import playbook from start to end for a single AKS cluster, then iterating over 100 AKS clusters, will take 500m or 8.3 hours, as a rough estimate. Enabling parallism will reduce the total time. Currently, the import procedure follows the out of the box import procedure, using a Service Account with cluster-admin rolebinding to generate the managed cluster namespace, and create the Managed Cluster CR. The best practice is always to use the mininal privilege as possible, so we question, if customers in general are ok with using a cluster-admin role to import all their clusters. An alternative import procedure is available, that uses a generic import payload to be applied on the AKS cluster, and a service account with minimal privledge. This requires the ACM hub to be configured to generate a generic payload. Using this procedure will simplify the import procedure by not having to access the ACM hub cluster during the import procedure. We would just need to access the target cluster. This alternative process will still be a Day 2 operation.","title":"Importing AKS Clusters"},{"location":"design/#cleaning-up-unknown-managed-clusters","text":"Today, most of the managed clusters that we see with Unknown states are clusters that have their managed applications deleted by customers. The current number has been manageable by manually detaching the clusters through the UI. To automate this task, we can follow this psuedo code: get the list of managed applications from the dynamic inventory get the list of managed clusters by their label, managed applications. the obsolete managed clusters will be the managed clusters with labels not in the dyanmic inventory list of managed applications.","title":"Cleaning Up Unknown Managed Clusters"},{"location":"design/#backup-and-restore-disaster-recovery","text":"TBD","title":"Backup and Restore | Disaster Recovery"},{"location":"design/#upgrade-ocp","text":"We are following the following OCP upgrade policy: The ACM SRE maintenance window is between Sunday night (CST) and Wednesday morning (CST). We are following OpenShift stable upgrade channel OCP minor version release upgrades will happen during this window. We will deploy the latest version, weekly. OCP major version release upgrades will happen during this window. We will upgrade to the next release, 1 week after it is available. Development stage is upgraded first. Production(s) stage is upgraded next.","title":"Upgrade - OCP"},{"location":"design/#gitops-procedure-for-upgrading-ocp","text":"This is driven by an ACM Policy that applies the expected ClusterVersion Custom Resource. Once the manifest is applied, OCP will orchestrate the update until it finishes.","title":"Gitops procedure for upgrading OCP"},{"location":"design/#upgrade-acm","text":"The upgrade of RHACM will happen during the upgrade maintenance window from Sunday night (CST) to Wedneday morning (CST). This allows us to provide business hour support across the maintenance windown. We will upgrade ACM on the maintenance window following the release of RHACM. Development stage is upgraded first. Production(s) stage is upgraded next.","title":"Upgrade - ACM"},{"location":"design/#gitops-procedure-for-upgrading-rhacm-24-to-rhacm-25","text":"There is a known blocker issue for upgrading RHACM from 2.4 to 2.5 when the cluster proxy addon is enabled. Before upgrading from 2.4 to 2.5 we also need to disable the backup feature. The following steps includes the workaround to allow the upgrade with Gitops. Create PR update MultiClusterHub CR to disable proxy addon spec : enableClusterProxyAddon : false Create PR to update ACM subscription spec : channel : release-2.5 installPlanApproval : Manual name : advanced-cluster-management source : redhat-operators sourceNamespace : openshift-marketplace Login to the OCP console to review and approve the install plan for release 2.5 upgrade. After the RHACM 2.5 upgrade completes successfully, create PR to update MultiClusterHub CR to re-enable the cluster proxy addon. apiVersion : operator.open-cluster-management.io/v1 kind : MultiClusterHub metadata : name : multiclusterhub namespace : open-cluster-management spec : overrides : components : - enabled : true name : cluster-proxy-addon - enabled : true name : multiclusterhub-repo - enabled : true name : search - enabled : true name : management-ingress - enabled : true name : console - enabled : true name : insights - enabled : true name : grc - enabled : true name : cluster-lifecycle - enabled : true name : volsync - enabled : true name : multicluster-engine - enabled : false name : cluster-backup Manually delete the secret cluster-proxy-signer and let cluster-proxy-addon-manager to refresh it. oc delete secret cluster-proxy-signer -n open-cluster-management","title":"Gitops procedure for upgrading RHACM 2.4 to RHACM 2.5"},{"location":"design/#alerts-and-alert-management","text":"TBD","title":"Alerts and Alert Management"},{"location":"design/#observability","text":"TBD","title":"Observability"},{"location":"design/#certificate-management","text":"Certificates on the cluster are signed by Let's Encrypt CA for api and default ingress (*.apps). Certificates are handled by cert-manager which is configured to use DNS-01 challenge. This is the reason for having a DNS Public Zone . Certificates lasts 90 days and are automatically renewed every 30 days. An ACM policy monitors the certificate expiration and local cluster will pass to non-compliant status if there is any certificate which expire in less than 25 days.","title":"Certificate management"},{"location":"fleet-health/","text":"Fleet Health Determine the Multiple Fleet Health When you focus on one Fleet, say the DEV fleet, it is easy to connect and walk through the console pages to determine the current state of the DEV fleet. Add in the production ACM1 fleet, and now its twice the cli commands, and twice the console page views. Let me record here the list of steps for manual visual inspection: enable sshuttle to acm1 verify the ACM console shows the expected managed clusters verify the OBS dashboard shows the extected managed clusters verify policies have no violations verify argocd applications are all green verify the OCP version verify the ACM version enable sshuttle to acm5 verify login through sso verify the ACM console has 1 managed cluster (local-cluster) verify that only 1 policy exists verify argocd is running, but doesn't have the users fix, need to be kube-admin to see applications verify cert-manager is not running, pending dns update, this application will be rolled out here verify OCP version 4.10.11 verify ACM version 2.5.0 Determining ACM Health The Red Hat Advanced Cluster Management product is delivered as an Operator Hub based operator. The Operator is deployed with a Subscription defintion. The instance Operand manifest is applied, requiring only a reference to an image pull secret. In the 2.4 release, we can verify the state of the ACM deployment by monitoring the status output of the multiclusterhub CR. The multiclusterhub CR reflects the state of the MulticlusterHub Operator , or MCH . If all the ACM components are successfully deployed, then the final status of the multiclusterhub CR will be Running . A state of Progressing indicates that the install process has not yet completed. A Blocked status indicates that the install or upgrade process was interrupted due to missing requirement or conflict that prevents the process from starting. In the 2.5 release, the cluster-lifecycle subcomponent was separated out as a standalone product component called Multicluster Engine , with a short name short name of MCE . When we deploy RHACM 2.5, both product component operators will be deployed. The Multiclusterhub operator and the Multicluster Engine operator. The MCH will refect the same status as before, with Running indicating sucessful completion of install or upgrade. The MCE will show a status of Available to indicate that its install/upgrade processing has completed successfully. graph LR MCH[MCH] -->|Subscribe| APPSUB_A(APPSUB_A) MCH[MCH] -->|Subscribe| APPSUB_B(APPSUB_B) MCH[MCH] -->|Subscribe| APPSUB_C(APPSUB_C) MCH[MCH] -->|Subscribe| APPSUB_D(APPSUB_D) MCH[MCH] -->|Subscribe| APPSUB_E(APPSUB_E) APPSUB_A[APPSUB_A] --> HELM_A[HELM_A] APPSUB_B[APPSUB_B] --> HELM_B[HELM_B] APPSUB_C[APPSUB_C] --> HELM_C[HELM_C] APPSUB_D[APPSUB_D] --> HELM_D[HELM_D] APPSUB_E[APPSUB_E] --> HELM_E[HELM_E] MCE[MCE] %% B --> C{Let me think} %% C -->|One| D[Laptop] %% C -->|Two| E[iPhone] %% C -->|Three| F[fa:fa-car Car] The MCH and MCE reflect the deploy time status of RHACM. We can derive the instantious pod state weither they are running or crashing, and historically if the pods have a history of restarting. From this we cannot tell if the internal function are nominal. Because we use Openshift Gitops to deploy RHACM as an ArgoCD application, we also inspect ArgoCD to determine the immediate state of the ACM component deployment (See Image 1 above). To determine ACM Heath from metrics and utilization, the Openshift Monitoriing components can be inspected. If ACM Observability is enabled, then the ACM Hub (local-cluster) is aggregated with the rest of the fleet data and available in the aggregate Grafana dashboard. What is needed We need to understand the API latency on each ACM subcomponent. We need to be able to measure ACM to determine SLO. The fact that ACM is a workload running on top of Openshift, knowing the k8s apiserver SLO, does not necessarily reflect the SLO for the ACM component api. There is no specific dashboard that shows data for the ACM components. Up to now, we manually create the view, without ever persisting the views to a reusable custom dashboard. Determine Fleet Health In order to review the fleet health or status, we can inspect ACM and Observability Grafana dashboard. The ACM cluster view indicates the current state of imported clusters, if they are Ready , or not Ready . The Goverance and Application views indicates the state of policy compliance and applications respectiveliy, across the fleet. Finally, the Observability dashboard aggregates metrics data from the fleet in a single pane of glass. Note There is one important notion that our work has highlighted. ACM as a system does not represent the source of truth clusters in the enterprise. We can import clusters from different cloud providers and even on-prem sources, but those providers will be the source of truth of their domain. ACM can span providers, but still, does not represent a source of truth. ACM is only aware of what is given it. Clusters can disappear. ACM does have the capability to create new clusters via the hive component or assisted installer. What is needed The SRE will need to define custom dashboards to display views specific to their needs. Workload metrics data is collected, but the out of the box Openshift dashboards is a general view. You workload components may not export useful data, so you will likely go through a cycle of improvement, and re-render dashboard view. Since we started with AOC/AAP, we have not produced any new custom dashboards, beyond the AAP golden signals. The process to create a custom dashboard, and exported out to source repo, and imported back in is captured as a script. But wouldn't this be better if its rolled up in the Openshift cluster as an Openshift Pipeline (tekton).","title":"Fleet Health"},{"location":"fleet-health/#determine-the-multiple-fleet-health","text":"When you focus on one Fleet, say the DEV fleet, it is easy to connect and walk through the console pages to determine the current state of the DEV fleet. Add in the production ACM1 fleet, and now its twice the cli commands, and twice the console page views. Let me record here the list of steps for manual visual inspection: enable sshuttle to acm1 verify the ACM console shows the expected managed clusters verify the OBS dashboard shows the extected managed clusters verify policies have no violations verify argocd applications are all green verify the OCP version verify the ACM version enable sshuttle to acm5 verify login through sso verify the ACM console has 1 managed cluster (local-cluster) verify that only 1 policy exists verify argocd is running, but doesn't have the users fix, need to be kube-admin to see applications verify cert-manager is not running, pending dns update, this application will be rolled out here verify OCP version 4.10.11 verify ACM version 2.5.0","title":"Determine the Multiple Fleet Health"},{"location":"fleet-health/#determining-acm-health","text":"The Red Hat Advanced Cluster Management product is delivered as an Operator Hub based operator. The Operator is deployed with a Subscription defintion. The instance Operand manifest is applied, requiring only a reference to an image pull secret. In the 2.4 release, we can verify the state of the ACM deployment by monitoring the status output of the multiclusterhub CR. The multiclusterhub CR reflects the state of the MulticlusterHub Operator , or MCH . If all the ACM components are successfully deployed, then the final status of the multiclusterhub CR will be Running . A state of Progressing indicates that the install process has not yet completed. A Blocked status indicates that the install or upgrade process was interrupted due to missing requirement or conflict that prevents the process from starting. In the 2.5 release, the cluster-lifecycle subcomponent was separated out as a standalone product component called Multicluster Engine , with a short name short name of MCE . When we deploy RHACM 2.5, both product component operators will be deployed. The Multiclusterhub operator and the Multicluster Engine operator. The MCH will refect the same status as before, with Running indicating sucessful completion of install or upgrade. The MCE will show a status of Available to indicate that its install/upgrade processing has completed successfully. graph LR MCH[MCH] -->|Subscribe| APPSUB_A(APPSUB_A) MCH[MCH] -->|Subscribe| APPSUB_B(APPSUB_B) MCH[MCH] -->|Subscribe| APPSUB_C(APPSUB_C) MCH[MCH] -->|Subscribe| APPSUB_D(APPSUB_D) MCH[MCH] -->|Subscribe| APPSUB_E(APPSUB_E) APPSUB_A[APPSUB_A] --> HELM_A[HELM_A] APPSUB_B[APPSUB_B] --> HELM_B[HELM_B] APPSUB_C[APPSUB_C] --> HELM_C[HELM_C] APPSUB_D[APPSUB_D] --> HELM_D[HELM_D] APPSUB_E[APPSUB_E] --> HELM_E[HELM_E] MCE[MCE] %% B --> C{Let me think} %% C -->|One| D[Laptop] %% C -->|Two| E[iPhone] %% C -->|Three| F[fa:fa-car Car] The MCH and MCE reflect the deploy time status of RHACM. We can derive the instantious pod state weither they are running or crashing, and historically if the pods have a history of restarting. From this we cannot tell if the internal function are nominal. Because we use Openshift Gitops to deploy RHACM as an ArgoCD application, we also inspect ArgoCD to determine the immediate state of the ACM component deployment (See Image 1 above). To determine ACM Heath from metrics and utilization, the Openshift Monitoriing components can be inspected. If ACM Observability is enabled, then the ACM Hub (local-cluster) is aggregated with the rest of the fleet data and available in the aggregate Grafana dashboard.","title":"Determining ACM Health"},{"location":"fleet-health/#what-is-needed","text":"We need to understand the API latency on each ACM subcomponent. We need to be able to measure ACM to determine SLO. The fact that ACM is a workload running on top of Openshift, knowing the k8s apiserver SLO, does not necessarily reflect the SLO for the ACM component api. There is no specific dashboard that shows data for the ACM components. Up to now, we manually create the view, without ever persisting the views to a reusable custom dashboard.","title":"What is needed"},{"location":"fleet-health/#determine-fleet-health","text":"In order to review the fleet health or status, we can inspect ACM and Observability Grafana dashboard. The ACM cluster view indicates the current state of imported clusters, if they are Ready , or not Ready . The Goverance and Application views indicates the state of policy compliance and applications respectiveliy, across the fleet. Finally, the Observability dashboard aggregates metrics data from the fleet in a single pane of glass. Note There is one important notion that our work has highlighted. ACM as a system does not represent the source of truth clusters in the enterprise. We can import clusters from different cloud providers and even on-prem sources, but those providers will be the source of truth of their domain. ACM can span providers, but still, does not represent a source of truth. ACM is only aware of what is given it. Clusters can disappear. ACM does have the capability to create new clusters via the hive component or assisted installer.","title":"Determine Fleet Health"},{"location":"fleet-health/#what-is-needed_1","text":"The SRE will need to define custom dashboards to display views specific to their needs. Workload metrics data is collected, but the out of the box Openshift dashboards is a general view. You workload components may not export useful data, so you will likely go through a cycle of improvement, and re-render dashboard view. Since we started with AOC/AAP, we have not produced any new custom dashboards, beyond the AAP golden signals. The process to create a custom dashboard, and exported out to source repo, and imported back in is captured as a script. But wouldn't this be better if its rolled up in the Openshift cluster as an Openshift Pipeline (tekton).","title":"What is needed"},{"location":"operations/","text":"Operations On-Call Escalation Coverage Level 1: Normal Business Hours With SRE members in three separate timezones, we can support follow the sun SRE coverage, with normal business hours. 8h shifts starting from 8AM Xian Sunday to Friday, with Austin on call to 6PM For coverage on Sundays, if Paris team is excluded, we can still cover with 12h shift between Xian and Austin/Boston. In Pagerduty, we should have a primary schedule with starting times in each timezone at 8AM, ending at 5PM, except in Austin, where the end is 6PM. Level 2: Exceptions For shifts outside the normal work week (Saturday/Sunday) and PTO/Holidays, we'll have a sign up sheet. Exceptions are filled in Pagerduty manually. To implement these hours, we'll create a secondary schedule, and this scheule will be used to populate the dates from the signup sheet. Level 3: Catch All A final catch all schedule be created to handle any incidents that are not acknowledged, will be covered by this final group. PagerDuty Integration Flow This is our current integration flow to PagerDuty. graph LR AlertManager --> |Reciever| PagerDuty AlertManager --> |Reciever| Slack It is also possible to integrate Prometheus directly to PagerDuty. graph LR Prometheus --> |Webhook| PagerDuty PageDuty Configuration graph LR Service --> Escalation_Policy Schedule --> Escalation_Policy Service_AOC_ACM_DEV --> Service Service_AOC_ACM_NA --> Service Service_AOC_ACM_EMEA --> Service EventRule_AOC_ACM_NA --> Service_AOC_ACM_NA EventRule_AOC_ACM_DEV --> Service_AOC_ACM_DEV SRE_PRIMARY --> Schedule SRE_SECONDARY --> Schedule Event RuleSet We created incoming event source for the three environments: dev, prod-na, prod-emea. Each event source has their own integration-key , that is used in the alertmanager configuration. Each ruleset will be associated to the corresponding service: dev, prod-na, prod-emea. Shifts Currently, we have Primary schedule as 8h shifts. For the Secondary schedule, should this be changed to 12h shifts like other teams?","title":"Operations"},{"location":"operations/#on-call-escalation-coverage","text":"","title":"On-Call Escalation Coverage"},{"location":"operations/#level-1-normal-business-hours","text":"With SRE members in three separate timezones, we can support follow the sun SRE coverage, with normal business hours. 8h shifts starting from 8AM Xian Sunday to Friday, with Austin on call to 6PM For coverage on Sundays, if Paris team is excluded, we can still cover with 12h shift between Xian and Austin/Boston. In Pagerduty, we should have a primary schedule with starting times in each timezone at 8AM, ending at 5PM, except in Austin, where the end is 6PM.","title":"Level 1: Normal Business Hours"},{"location":"operations/#level-2-exceptions","text":"For shifts outside the normal work week (Saturday/Sunday) and PTO/Holidays, we'll have a sign up sheet. Exceptions are filled in Pagerduty manually. To implement these hours, we'll create a secondary schedule, and this scheule will be used to populate the dates from the signup sheet.","title":"Level 2: Exceptions"},{"location":"operations/#level-3-catch-all","text":"A final catch all schedule be created to handle any incidents that are not acknowledged, will be covered by this final group.","title":"Level 3: Catch All"},{"location":"operations/#pagerduty-integration-flow","text":"This is our current integration flow to PagerDuty. graph LR AlertManager --> |Reciever| PagerDuty AlertManager --> |Reciever| Slack It is also possible to integrate Prometheus directly to PagerDuty. graph LR Prometheus --> |Webhook| PagerDuty","title":"PagerDuty Integration Flow"},{"location":"operations/#pageduty-configuration","text":"graph LR Service --> Escalation_Policy Schedule --> Escalation_Policy Service_AOC_ACM_DEV --> Service Service_AOC_ACM_NA --> Service Service_AOC_ACM_EMEA --> Service EventRule_AOC_ACM_NA --> Service_AOC_ACM_NA EventRule_AOC_ACM_DEV --> Service_AOC_ACM_DEV SRE_PRIMARY --> Schedule SRE_SECONDARY --> Schedule","title":"PageDuty Configuration"},{"location":"operations/#event-ruleset","text":"We created incoming event source for the three environments: dev, prod-na, prod-emea. Each event source has their own integration-key , that is used in the alertmanager configuration. Each ruleset will be associated to the corresponding service: dev, prod-na, prod-emea.","title":"Event RuleSet"},{"location":"operations/#shifts","text":"Currently, we have Primary schedule as 8h shifts. For the Secondary schedule, should this be changed to 12h shifts like other teams?","title":"Shifts"}]}